# -*- coding: utf-8 -*-
"""pytorch_tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kj5R_EYxKTXdjcqjWLDqlvVckj23QICY

Import modules and set up the environment. When using colab, all common libraries such as pytorch, numpy, etc. are already installed in the server, you don't need to worry about environment setup and directly import the libraries.
"""

import torch
from torch import Tensor
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import time

import torch
import torch.nn as nn

# Hyperparameters
num_epochs = 10
batch_size = 128 # you can lower this to 64 or 32 to help speed up training.
learning_rate = 1e-3

"""Now let's select GPU to train the model by setting the device parameter. Pytorch provides a free GPU for you to use."""

# Device configuration
if torch.cuda.is_available():
    device = torch.device('cuda')
elif torch.backends.mps.is_available():
    device = torch.device('mps')
else:
    device = torch.device('cpu')
print(f"Using device: {device}")

"""Define the data transformations and load the CIFAR-10 dataset"""

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

class Conv2d_custom(nn.Conv2d):
    def __init__(self, in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation = 1, groups = 1, bias = True, padding_mode = "zeros", device=None, dtype=None):
        super().__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)
    
    def forward(self, input: Tensor) -> Tensor:
        # add noise
        noise = torch.normal(mean=0, std=0.1, size=input.size(), device=device)
        return self._conv_forward(input + noise, self.weight, self.bias)

"""Define a custom simple neural network (LeNet-like). You need to read Pytorch's documentation of nn.Conv2d to understand waht do the input parameters mean. Here we define a simple 5-layer CNN with 2 convolution layers and 2 fully connected layers."""


class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        # First channel must set to 3 for this dataset as it has three color channels
        self.conv1 = Conv2d_custom(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv2 = Conv2d_custom(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)
        self.conv3 = Conv2d_custom(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.norm1 = nn.BatchNorm2d(num_features=32)
        self.norm2 = nn.BatchNorm2d(num_features=32)
        self.norm3 = nn.BatchNorm2d(num_features=64)
        self.fc = nn.Linear(64 * 4 * 4, 10)

    def forward(self, x):
        # First layer
        # 32x32x3 input
        x = self.conv1(x)
        x = self.pool(x)
        x = self.norm1(x)
        x = torch.relu(x)
        # 16x16x32 output

        # Second layer
        # 16x16x32 input
        x = self.conv2(x)
        x = self.pool(x)
        x = self.norm2(x)
        x = torch.relu(x)
        # 8x8x32 output

        # Third layer
        # 8x8x32 input
        x = self.conv3(x)
        x = self.pool(x)
        x = self.norm3(x)
        x = torch.relu(x)
        # 4x4x64 output

        # Flatten to 1D
        # 4x4x64 input
        x = torch.flatten(x, start_dim=1)
        x = self.fc(x)
        # 10 outputs

        return x

net = SimpleNet().to(device) #.to(device) send the define neural network to the specified device
net.load_state_dict(torch.load('./lab1/part1a.pth', map_location=device))

"""Define the loss function and optimizer. Cross entropy loss is typically the default choice for classification problems. Again you can check Pytorch's documentation to see what optimizers you can use (there are plenty of them). Some common choices are SGD and adam."""

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=learning_rate) #0.001 is the default LR for Adam.

print(net)

"""Test the trained network, typically called 'inference'."""

net.eval()
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        images = images.to(device)
        labels = labels.to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')

"""Finally, save the network"""
torch.save(net.state_dict(), './lab1/part1c.pth')
